---
title: "multipset5"
author: "Daniel Kim"
date: "3/31/2020"
output:
  pdf_document: default
  html_document: default
---

```{r}
college <- read.csv("~/Downloads/Universities.csv")
college
```

1)
The variables seem to be mainly continuous variable so I think Euclidean distance as a metric would work in order to measure distance between points. The standard deviations of the variables seem to vary so I will scale/standardize my data in part 2.

```{r}
var(college[,c(2, 3, 4, 5, 6, 7)])
```


2)
```{r}
collegenorm <- college[,c("SAT","Top10","Accept","SFRatio","Expenses","Grad")]
rownames(collegenorm) <- college[,1]
collegenorm <- scale(na.omit(collegenorm)) # scaling my variables

#get the distance matrix
dist1 <- dist(collegenorm, method="euclidean")


clust1 <- hclust(dist1)

#draw the dendrogram
plot(clust1,labels= rownames(collegenorm), cex=0.6, xlab="",ylab="Distance",main="Clustering of Universities")
rect.hclust(clust1, k =5)
```
Using euclidean distance metrics and complete linkage method, there appears to be perhaps 5 main groups asssociated with the clustering of universities.

```{r}
dist2 <- dist(collegenorm, method="manhattan")
clust2 <- hclust(dist2)
plot(clust2,labels= rownames(collegenorm), cex=0.6, xlab="",ylab="Distance",main="Clustering of Universities")
rect.hclust(clust2, k = 5)
```
Using manhattan distance metrics and complete linkage method, there seems to be a bunch larger abudnace of smaller subgroups, mot likely attributted to the non-euclidean distance metric

```{r}
dist3 <- dist(collegenorm, method="euclidean")
clust3 <- hclust(dist3, method = "ward.D")
plot(clust3,labels= rownames(collegenorm), cex=0.6, xlab="",ylab="Distance",main="Clustering of Universities")
rect.hclust(clust3, k = 3)
```
Using euclidean distance metrics and ward linkage method, you could argue for maybe 3 main groups, attributted to the ward method of minimizing sum of squares.


```{r}
dist4 <- dist(collegenorm, method="euclidean")
clust4 <- hclust(dist4, method = "average")
plot(clust4,labels= rownames(collegenorm), cex=0.6, xlab="",ylab="Distance",main="Clustering of Universities")
rect.hclust(clust4, k = 4)
```
Using euclidean distance metrics and average linkage method, you could argue for around 4 cluster groups. This method is a space conserving method which could be the reason why.


3)
```{r}
source("http://reuningscherer.net/stat660/R/HClusEval.R.txt")
hclus_eval(collegenorm, dist_m = 'euclidean', clus_m = 'ward', plot_op = T)
```

It looks like there are around 5 cluster groups.  While the RMSSTD and CD lines offer little information, the points where the RSQ and SPRSQ curves start to level out is around 5 cluster groups.

4)
```{r}
km1 <- kmeans(collegenorm,centers=5)
km1

for (i in 1:5){
  print(paste("Universities in Cluster ",i))
  print(college$University[km1$cluster==i])
  print (" ")
}
```

```{r}
set.seed(123)
library(factoextra)

fviz_nbclust(collegenorm, kmeans, method = "wss")
```


```{r}
kdata <- collegenorm
n.lev <- 15  #set max value for number of clusters k

# Calculate the within groups sum of squared error (SSE) for the number of cluster solutions selected by the user
wss <- rnorm(10)
while (prod(wss==sort(wss,decreasing=T))==0) {
  wss <- (nrow(kdata)-1)*sum(apply(kdata,2,var))
  for (i in 2:n.lev) wss[i] <- sum(kmeans(kdata, centers=i)$withinss)}

# Calculate the within groups SSE for 250 randomized data sets (based on the original input data)
k.rand <- function(x){
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

rand.mat <- matrix(0,n.lev,250)

k.1 <- function(x) { 
  for (i in 1:250) {
    r.mat <- as.matrix(suppressWarnings(k.rand(kdata)))
    rand.mat[,i] <- r.mat}
  return(rand.mat)
}

# Same function as above for data with < 3 column variables
k.2.rand <- function(x){
  rand.mat <- matrix(0,n.lev,250)
  km.rand <- matrix(sample(x),dim(x)[1],dim(x)[2])
  rand.wss <- as.matrix(dim(x)[1]-1)*sum(apply(km.rand,2,var))
  for (i in 2:n.lev) rand.wss[i] <- sum(kmeans(km.rand, centers=i)$withinss)
  rand.wss <- as.matrix(rand.wss)
  return(rand.wss)
}

k.2 <- function(x){
  for (i in 1:250) {
    r.1 <- k.2.rand(kdata)
    rand.mat[,i] <- r.1}
  return(rand.mat)
}

# Determine if the data data table has > or < 3 variables and call appropriate function above
if (dim(kdata)[2] == 2) { rand.mat <- k.2(kdata) } else { rand.mat <- k.1(kdata) }

# Plot within groups SSE against all tested cluster solutions for actual and randomized data - 1st: Log scale, 2nd: Normal scale

xrange <- range(1:n.lev)
yrange <- range(log(rand.mat),log(wss))
plot(xrange,yrange, type='n', xlab='Cluster Solution', ylab='Log of Within Group SSE', main='Cluster Solutions against Log of SSE')
for (i in 1:250) lines(log(rand.mat[,i]),type='l',col='red')
lines(log(wss), type="b", col='blue')
legend('topright',c('Actual Data', '250 Random Runs'), col=c('blue', 'red'), lty=1)

```

There seems to be around 5 groups when looking at the k-means result. Looking at the SSE plotted against the cluster groups for the actual data against 250 random runs, the point where the distance between the two stops cahnging is around 5 groups.

5)

Based on the variety of dendrograms, we would reason that there should be somewhere around 4-6 groups among our data when clustering. The R square and semi-partial R squared graphs seem to place the number of groups around 5 and this is supplanted by the k-means data where we can graph the sum of squares within groups against cluster groups to see that the number of groups present seems to be around 5.